---
title: MIT 6.824
tags: Notes Computer-Science
article_header:
  type: 
  image:
    src: 
---

MIT 6.824 Distributed Systems learning notes.

<!--more-->

# Intro

## What is distributed system

- Multiple cooperating computers
- Storage for big web sites, MapReduce, peer-to-peer sharing, ...


## Why distributed system

- Connect physically separated machines (close to external entities)
- Increase capacity via parallelism
- Tolerate faults via replication
- Achieve security via isolation




## Current challenges

- Many concurrent parts, complex interactions
- Must cope with partial failure
- Tricky to realize performance potential




## Ideas

- Replicated servers: If one server crashes, can proceed using the other(s)
   - Apps can make progress despite failures(Availability), or come back to life when failures are repaired(Recoverability)
   - Very hard to get right, as server may not have crashed, but just unreachable for some reasons and still serving requests from clients
- General-purpose infrastructure needs well-defined behavior for consistency
   - "Replica" servers are hard to keep identical
- Scalable throughput
   - Scaling gets harder as N grows. (slowest-of-N latency, Non-parallelizable code, Bottlenecks from shared resources, ...)
   - Some performance problems aren't easily solved by scaling. (quick response time for a single user request, all users want to update the same data, ...)
- Trade-off between fault-tolerance, consistency, and performance
   - Strong fault tolerance and consistency require communication
   - Many designs provide only weak consistency, to gain speed

# RPC and Threads
## Why Golang?

- Good support for threads and RPC
- Type safe
- Automatic garbage collection

## Thread

### Introduction

- Useful structuring tool, but can be tricky
- Go calls them goroutines; everyone else calls them threads
- Important characteristics
   - Allow one program to do many things at once
   - Each thread executes serially, just like an ordinary non-threaded program
   - Threads share memory, while each thread includes some per-thread state: program counter, registers, stack

### Why use thread?

- Express concurrency, which is fundamental in distributed system
- Multicore performance, execute code in parallel on several cores
- I/O concucrrency
   - Client sends requests to many servers in parallel and waits for replies
   - Server processes multiple client requests; each request may block
   - While waiting for the disk to read data for client X, process a request from client Y



### Challenge

- Race condition, especially for shared data, this may cause data inconsistency
   - Use lock (Go's sync.Mutex)
- Coordination, for example one thread is producing data while the other is consuming 
   - Use Go channels or sync.Cond or WaitGroup
- Deadlock
   - Cycles via locks and/or communication (RPC, Go channel, ...)


## RPC

### Introduction

- Remote Procedure Call
- Goal: easy-to-program client/server communication
   - hide details of network protocols
   - convert data (strings, arrays, maps, &c) to "wire format"

### Failure

- What does a failure look like to the client RPC library
   - Client never sees a response from the server
   - Client does *not* know if the server saw the request!
- "At least one"(Best-effort)
   -  Call() waits for response for a while. If none arrives, re-send the request. Do this a few times, then give up and return an error
   - Only suitable for read-only operations, or operations that do nothing if repeated
- "At most one"
   - Server RPC code detects duplicate requests, then returns previous reply instead of re-running handler
   - Client includes unique ID (XID) with each request, uses same XID for re-send
   - Some potential issues
      - Two clients use the same XID (Big random number? Combine with sequence number?)
      - Server must eventually discard info about old RPCs (Similar to TCP Seq and Ack)
      - Handle duplicate request while original is still executing ("Pending flag", wait or ignore)
- "Exactly one"
   - Unbounded retries
   - Duplicate detection
   - Fault-tolerant service

# Google File System

## Why is distributed storage hard?

- High performance -> shard data over many servers
- Many servers -> constant faults
- Fault tolerance -> replication
- Replication -> potential inconsistencies
- Better consistency -> low performance


## Ideal Consistency

- Same behavior as a single server
- Each server should:
   - Use disk storage
   - Execute client operations one at a time
   - Read reflect previous write, even if server crashes or restarts

## Why is GFS successful?

- Not the basic ideas of distribution, sharding, fault-tolerance
   - Huge scale
   - Used in industry, real-world experience
   - Successful in weak consistency with single master node
- Context
   - Big & fast unified system
   - Global: any client can read any file, which allows sharing of data among applications
   - Automatic "sharding" of each file over many servers/disks for parallel performance and freeing space
   - Automatic recovery from failures
   - Aimed at sequential access to huge files; read or append

## Overall structure

- Clients (library, RPC -- but not visible as a UNIX FS)
- File split into independent 64 MB chunks
- Chunk servers, each chunk replicated on 3; every file's chunks are spread over the chunk servers
- Single master, and master replicas
- Separation of naming (master) from storage (chunkserver): master deals with naming, chunkservers with data

## Master

- In RAM (for speed, must be smallish)
- Main work
   - File name -> array of chunk handles
   - Chunk handle -> version, list of chunkservers, primary & secondary replica, lease time
- Also store something on disk
   - Log
   - Checkpoint

## GFS Consistency

- If the primary tells a client that a record append succeeded, then any reader that subsequently opens the file and scans it will see the appended record somewhere
- How GFS fulfils this guarantee:
   - crash, crash+reboot, crash+replacement, message loss, partition
- If to achieve stricter consistency?
   - Primary should detect duplicate client write requests, or client should not issue them
   - To avoid client reading from stale ex-secondary, either all client reads must go to primary, or secondaries must also have leases
   - All secondaries should complete each write, or none
   - If primary crashes, some replicas may be missing the last few ops. New primary must talk to all replicas to find all recent ops, and sync with secondaries.

## Disadvantage

- Single master performance -> ran out of RAM and CPU
- Chunkservers not very efficient for small files
- Lack of automatic fail-over to master replica
- Maybe consistency was too relaxed
