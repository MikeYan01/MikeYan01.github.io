---
title: Netty
tags: Notes Computer-Science Distributed-Systems
article_header:
  type: 
  image:
    src: 
---

Netty learning notes.

<!--more-->

## 简介

Netty 是一个 NIO client-server(客户端服务器)框架，功能强大，扩展能力强，性能高。Dubbo底层使用Netty框架进行网络通信。


架构图：

![netty.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/netty.png)

- 绿色的部分Core核心模块，包括零拷贝、API库、可扩展的事件模型。
- 橙色部分Protocol Support协议支持，包括Http协议、webSocket、SSL(安全套接字协议)、谷歌Protobuf协议、zlib/gzip压缩与解压缩、Large File Transfer大文件传输等等。
- 红色的部分Transport Services传输服务，包括Socket、Datagram、Http Tunnel等等。



## 零拷贝


Netty的零拷贝是在用户态(Java层面)的，更多是数据操作的优化，在传输数据时，最终处理的数据会需要对单个传输的报文，进行组合和拆分。主要体现在五个方面：


1. Netty的接收和发送ByteBuffer使用直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用JVM的堆内存进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于使用直接内存，消息在发送过程中多了一次缓冲区的内存拷贝
2. Netty的文件传输调用FileRegion包装的transferTo方法，可以直接将文件缓冲区的数据发送到目标Channel，避免通过循环write方式导致的内存拷贝问题
3. Netty提供CompositeByteBuf类, 可以将多个ByteBuf合并为一个逻辑上的ByteBuf, 避免了各个ByteBuf之间的拷贝
4. 通过wrap操作, 我们可以将byte[]数组、ByteBuf、ByteBuffer等包装成一个Netty ByteBuf对象, 进而避免拷贝操作
5. ByteBuf支持slice操作，可以将ByteBuf分解为多个共享同一个存储区域的ByteBuf, 避免内存的拷贝。


## NIO与IO多路复用


见操作系统笔记


## 线程模型和线程组


Netty中的三种线程模型：


1. Reactor 单线程模型，指的是所有的 IO 操作都在同一个 NIO 线程上面完成，适合一些小容量应用场景

    ![reactor-1.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/reactor-1.png)

    由于 Reactor 模式使用的是异步非阻塞 IO，所有的 IO 操作都不会导致阻塞，理论上一个线程可以独立处理所有 IO 相关的操作，然而性能上无法支撑，即便 NIO 线程的 CPU 负荷达到 100%，也无法满足海量消息的编码、解码、读取和发送；且线程负载过重之后，处理速度将变慢，这会导致大量客户端连接超时，超时之后往往会进行重发，这更加重了 NIO 线程的负载



2. Rector 多线程模型，与单线程模型最大的区别就是有一组 NIO 线程处理 IO 操作

    ![reactor-2.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/reactor-2.png)

    - 一个 NIO 线程 -Acceptor 线程用于监听服务端
    - 网络 IO 操作 - 读、写等由一个 NIO 线程池负责
    - 1 个 NIO 线程可以同时处理 N 条链路，但是 1 个链路只对应 1 个 NIO 线程，防止发生并发操作问题。

    并发海量客户端连接时，或者服务端需要对客户端握手进行安全认证时，单独一个 Acceptor 线程可能会存在性能不足问题


3. 主从多线程模型，服务端用于接收客户端连接的不再是个 1 个单独的 NIO 线程，而是一个独立的 NIO 线程池，可以解决 1 个服务端监听线程无法有效处理所有客户端连接的性能不足问题

    ![reactor-3.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/reactor-3.png)

    - 从主线程池中随机选择一个 Reactor 线程作为 Acceptor 线程，用于绑定监听端口，接收客户端连接
    - Acceptor 线程接收客户端连接请求之后创建新的 SocketChannel，将其注册到主线程池的其它 Reactor 线程上，由其负责接入认证、IP 黑白名单过滤、握手等操作；
    - 业务层的链路正式建立，将 SocketChannel 从主线程池的 Reactor 线程的多路复用器上摘除，重新注册到 Sub 线程池的线程上，用于处理 I/O 的读写操作。


    通常情况下，服务端的创建是在用户进程启动的时候进行，因此一般由 Main 函数或者启动类负责创建，服务端的创建由业务线程负责完成。在创建服务端的时候实例化了 2 个 事件循环组，1 个事件循环组实际就是一个事件循环线程组，负责管理事件循环的申请和释放。事件循环组管理的线程数可以通过构造函数设置，如果没有设置，默认取 -Dio.netty.eventLoopThreads，如果该系统参数也没有指定，则为可用的 CPU 内核数 × 2。

    bossGroup 线程组实际就是 Acceptor 线程池，负责处理客户端的 TCP 连接请求，如果系统只有一个服务端端口需要监听，则建议 bossGroup 线程组线程数设置为 1。workerGroup 是真正负责 I/O 读写操作的线程组，通过 ServerBootstrap 的 group 方法进行设置，用于后续的 Channel 绑定。


## 执行流程图


服务器端：

![netty-pipeline-1.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/netty-pipeline-1.png)

客户端：

![netty-pipeline-2.png](https://raw.githubusercontent.com/MikeYan01/mikeyan01.github.io/master/assets/images/DistributedSystems/netty-pipeline-2.png)
